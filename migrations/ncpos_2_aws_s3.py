import boto3
from botocore.client import Config
import os
from datetime import datetime, timedelta
import logging
from dotenv import load_dotenv
import time
import concurrent.futures
from itertools import islice

load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ í‚¤ ê°’ë“¤ì„ ê°€ì ¸ì˜´
ncp_access_key = os.getenv('NCP_ACCESS_KEY')
ncp_secret_key = os.getenv('NCP_SECRET_KEY')
aws_access_key = os.getenv('AWS_ACCESS_KEY')
aws_secret_key = os.getenv('AWS_SECRET_KEY')
# ë²„í‚· ì´ë¦„ë„ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬
ncp_bucket_name = os.getenv('NCP_BUCKET_NAME')
aws_bucket_name = os.getenv('AWS_BUCKET_NAME')

# NCP ë²„í‚· ëª©ë¡ ì •ì˜
NCP_BUCKETS = [
    "dentop02"
]

class MigrationHandler:
    def __init__(self, source_bucket, dest_bucket):
        self.setup_clients()
        self.setup_logging()
        self.source_bucket = source_bucket
        self.dest_bucket = dest_bucket
        self.start_time = None
        self.stats = {
            'total': 0,
            'success': 0,
            'skipped': 0,
            'failed': 0,
            'total_bytes': 0,
            'transferred_bytes': 0
        }

    def setup_clients(self):
        """NCPì™€ AWS í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        self.ncp_client = boto3.client(
            's3',
            aws_access_key_id=ncp_access_key,
            aws_secret_access_key=ncp_secret_key,
            endpoint_url='https://kr.object.ncloudstorage.com',
            config=Config(signature_version='s3v4')
        )
        
        self.aws_client = boto3.client(
            's3',
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key,
            region_name='ap-northeast-2'
        )

    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f'logs/migration_{timestamp}.log'
        
        # logs ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs('logs', exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def list_objects(self, prefix: str = "") -> list:
        """NCP ë²„í‚·ì˜ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        try:
            objects = []
            paginator = self.ncp_client.get_paginator('list_objects_v2')
            
            for page in paginator.paginate(Bucket=self.source_bucket, Prefix=prefix):
                if 'Contents' in page:
                    objects.extend(page['Contents'])
            
            return objects
        except Exception as e:
            self.logger.error(f"Error listing objects: {str(e)}")
            raise

    def format_size(self, size):
        """ë°”ì´íŠ¸ í¬ê¸°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size < 1024.0:
                return f"{size:.2f} {unit}"
            size /= 1024.0

    def format_time(self, seconds):
        """ì´ˆ ë‹¨ìœ„ ì‹œê°„ì„ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return str(timedelta(seconds=int(seconds)))

    def migrate_object(self, obj: dict, retry_count: int = 3) -> bool:
        """ë‹¨ì¼ ê°ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ - AWS S3ì— ì—†ëŠ” ê²½ìš°ì—ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        object_key = obj['Key']
        
        try:
            # AWS S3ì— í•´ë‹¹ ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
            try:
                self.aws_client.head_object(
                    Bucket=self.dest_bucket,
                    Key=object_key
                )
                # ê°ì²´ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
                self.logger.info(f"Object already exists in S3, skipping: {object_key}")
                obj['migration_status'] = 'skipped'
                return True
            except:
                # ê°ì²´ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰
                for attempt in range(retry_count):
                    try:
                        # NCPì—ì„œ ê°ì²´ ë‹¤ìš´ë¡œë“œ
                        response = self.ncp_client.get_object(
                            Bucket=self.source_bucket,
                            Key=object_key
                        )
                        
                        # AWSì— ì—…ë¡œë“œ (í´ë” êµ¬ì¡° ìœ ì§€)
                        self.aws_client.upload_fileobj(
                            response['Body'],
                            self.dest_bucket,
                            object_key  # ì›ë³¸ ê²½ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ í´ë” êµ¬ì¡° ìœ ì§€
                        )
                        
                        self.logger.info(f"Successfully migrated: {object_key}")
                        return True
                        
                    except Exception as e:
                        self.logger.error(f"Error migrating {object_key}: {str(e)}")
                        if attempt == retry_count - 1:  # ë§ˆì§€ë§‰ ì‹œë„ì˜€ë‹¤ë©´
                            return False
                        
                        self.logger.info(f"Retrying... ({attempt + 1}/{retry_count})")
        
        except Exception as e:
            self.logger.error(f"Unexpected error with {object_key}: {str(e)}")
            return False
        
        return False

    def verify_buckets(self):
        """ì†ŒìŠ¤(NCP)ì™€ ëŒ€ìƒ(AWS) ë²„í‚·ì˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            # NCP ë²„í‚· í™•ì¸
            self.ncp_client.head_bucket(Bucket=self.source_bucket)
            # AWS ë²„í‚· í™•ì¸
            self.aws_client.head_bucket(Bucket=self.dest_bucket)
            self.logger.info(f"Verified both buckets exist - NCP: {self.source_bucket}, AWS: {self.dest_bucket}")
            return True
        except Exception as e:
            self.logger.error(f"Bucket verification failed: {str(e)}")
            return False

    def compare_objects(self, ncp_obj, aws_obj):
        """ë‘ ê°ì²´ì˜ ë©”íƒ€ë°ì´í„° ë¹„êµ"""
        if ncp_obj['Size'] != aws_obj['Size']:
            return False
        
        # ETag ë¹„êµ (MD5 ì²´í¬ì„¬)
        ncp_etag = ncp_obj['ETag'].strip('"')
        aws_etag = aws_obj['ETag'].strip('"')
        return ncp_etag == aws_etag

    def get_aws_objects(self, prefix: str = "") -> dict:
        """AWS S3 ë²„í‚·ì˜ ê°ì²´ ë¦¬ìŠ¤ï¿½ï¿½ï¿½ ì¡°íšŒ"""
        try:
            objects = {}
            paginator = self.aws_client.get_paginator('list_objects_v2')
            
            for page in paginator.paginate(Bucket=self.dest_bucket, Prefix=prefix):
                if 'Contents' in page:
                    for obj in page['Contents']:
                        objects[obj['Key']] = obj
            
            return objects
        except Exception as e:
            self.logger.error(f"Error listing AWS objects: {str(e)}")
            raise

    def analyze_migration_needs(self, prefix: str = ""):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìš”ì„± ë¶„ì„"""
        self.logger.info("Analyzing migration needs...")
        
        # NCPì™€ AWSì˜ ê°ì²´ ëª©ë¡ ì¡°íšŒ
        ncp_objects = {obj['Key']: obj for obj in self.list_objects(prefix)}
        aws_objects = self.get_aws_objects(prefix)
        
        total_objects = len(ncp_objects)
        existing_objects = 0
        different_objects = 0
        new_objects = 0
        total_size = 0
        
        for key, ncp_obj in ncp_objects.items():
            total_size += ncp_obj['Size']
            
            if key in aws_objects:
                if self.compare_objects(ncp_obj, aws_objects[key]):
                    existing_objects += 1
                else:
                    different_objects += 1
            else:
                new_objects += 1
        
        analysis = {
            'total_objects': total_objects,
            'existing_identical': existing_objects,
            'needs_update': different_objects,
            'new_objects': new_objects,
            'total_size': total_size
        }
        
        self.logger.info(
            f"\nMigration Analysis Results:\n"
            f"Total objects in NCP: {total_objects}\n"
            f"Already identical in AWS: {existing_objects}\n"
            f"Need update (different): {different_objects}\n"
            f"New objects to migrate: {new_objects}\n"
            f"Total size to migrate: {self.format_size(total_size)}\n"
        )
        
        return analysis

    def chunk_list(self, lst, chunk_size):
        """ë¦¬ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• """
        iterator = iter(lst)
        return iter(lambda: list(islice(iterator, chunk_size)), [])

    def migrate_chunk(self, objects):
        """ì²­í¬ ë‹¨ìœ„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì²˜ë¦¬"""
        results = {'success': 0, 'failed': 0, 'skipped': 0, 'transferred_bytes': 0}
        
        for obj in objects:
            try:
                if self.migrate_object(obj):
                    if obj.get('migration_status') == 'skipped':
                        results['skipped'] += 1
                    else:
                        results['success'] += 1
                        results['transferred_bytes'] += obj['Size']
                else:
                    results['failed'] += 1
            except Exception as e:
                self.logger.error(f"Error processing {obj['Key']}: {str(e)}")
                results['failed'] += 1
        
        return results

    def run_migration(self, prefix: str = ""):
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        self.start_time = time.time()
        objects = self.list_objects(prefix)
        self.stats['total'] = len(objects)
        self.stats['total_bytes'] = sum(obj['Size'] for obj in objects)
        
        self.logger.info(f"Starting migration of {self.stats['total']} objects")
        
        for i, obj in enumerate(objects, 1):
            if self.migrate_object(obj):
                if obj.get('migration_status') == 'skipped':
                    self.stats['skipped'] += 1
                else:
                    self.stats['success'] += 1
                    self.stats['transferred_bytes'] += obj['Size']
            else:
                self.stats['failed'] += 1
            
            # ì§„í–‰ë¥  ê³„ì‚° ë° ì¶œë ¥
            progress = (i / self.stats['total']) * 100
            elapsed_time = time.time() - self.start_time
            speed = self.stats['transferred_bytes'] / elapsed_time if elapsed_time > 0 else 0
            
            self.logger.info(
                f"Progress: {progress:.1f}% ({i}/{self.stats['total']}) | "
                f"Speed: {self.format_size(speed)}/s | "
                f"Elapsed: {self.format_time(elapsed_time)}\n"
                f"Success: {self.stats['success']}, "
                f"Skipped: {self.stats['skipped']}, "
                f"Failed: {self.stats['failed']}"
            )
        
        total_time = time.time() - self.start_time
        self.logger.info(
            f"\nMigration completed in {self.format_time(total_time)}\n"
            f"Total objects: {self.stats['total']}\n"
            f"Successfully migrated: {self.stats['success']}\n"
            f"Skipped (already exist): {self.stats['skipped']}\n"
            f"Failed: {self.stats['failed']}\n"
            f"Total size: {self.format_size(self.stats['total_bytes'])}\n"
            f"Transferred size: {self.format_size(self.stats['transferred_bytes'])}\n"
            f"Average speed: {self.format_size(self.stats['transferred_bytes'] / total_time)}/s"
        )

    def print_bucket_structure(self, prefix: str = ""):
        """ë²„í‚·ì˜ í´ë” êµ¬ì¡° ì¶œë ¥"""
        objects = self.list_objects(prefix)
        
        # í´ë” êµ¬ì¡°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        structure = {}
        
        for obj in objects:
            path_parts = obj['Key'].split('/')
            current = structure
            
            # ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì œì™¸í•œ ëª¨ë“  ë¶€ë¶„ì„ í´ë”ë¡œ ì²˜ë¦¬
            for part in path_parts[:-1]:
                if part not in current:
                    current[part] = {}
                current = current[part]
            
            # ë§ˆì§€ë§‰ ë¶€ë¶„ì„ íŒŒì¼ë¡œ ì²˜ë¦¬
            current[path_parts[-1]] = obj['Size']
        
        def print_structure(data, level=0):
            for key, value in sorted(data.items()):
                indent = "  " * level
                if isinstance(value, dict):
                    self.logger.info(f"{indent}ğŸ“ {key}/")
                    print_structure(value, level + 1)
                else:
                    self.logger.info(f"{indent}ğŸ“„ {key} ({self.format_size(value)})")
        
        self.logger.info(f"\nBucket structure for {self.source_bucket}:")
        print_structure(structure)

# ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    for bucket in NCP_BUCKETS:
        print(f"\nAnalyzing bucket structure: {bucket}")
        handler = MigrationHandler(
            source_bucket=bucket,
            dest_bucket=bucket
        )
        # ë¨¼ì € ë²„í‚· êµ¬ì¡° ì¶œë ¥
        handler.print_bucket_structure()
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
        handler.run_migration()
        print(f"Completed migration for bucket: {bucket}\n")